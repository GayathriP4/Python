{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GayathriP4/Python/blob/main/mp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpurJOdfV6iI",
        "outputId": "ee72e125-e7db-4794-9398-a8fbdb9b0632",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "#done once\n",
        "#open the url, grant permissions and then copy the authorization code provided \n",
        "#and paste in the box provided\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzs3zIzxCV2O",
        "outputId": "d54994db-dc7a-41b4-ed90-0f4a4f6a39df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        }
      },
      "source": [
        "#Setup \n",
        "#only done once\n",
        "#all required packages are downloaded\n",
        "!pip install -q wordcloud\n",
        "import wordcloud\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger') "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cN4hokUpCXFc"
      },
      "source": [
        "import os\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "from nltk.stem import PorterStemmer as PS\n",
        "from nltk.stem import WordNetLemmatizer as wnl\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "from nltk.corpus import wordnet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIUKqlzzCfcR"
      },
      "source": [
        "#opens the directory and returns path of all files\n",
        "\n",
        "def lfld(path,fp):\n",
        "    if os.path.isdir(path):\n",
        "        l = os.listdir(path)\n",
        "        for x in l:\n",
        "            tp = path + '/' + x\n",
        "            lfld(tp,fp)\n",
        "    elif os.path.isfile(path):\n",
        "        fp.append(path)\n",
        "        return\n",
        "    else: pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_-hV6Xx2d7U"
      },
      "source": [
        "#reads a file\n",
        "#calls preprocess function\n",
        "def readf(n):\n",
        "  f = open(n,'r',encoding='latin-1')\n",
        "  i = f.read()\n",
        "  f.close()\n",
        "  return i"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFaZ9fRFndlI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "b5a03d21-7494-4e3e-c72f-fbf08e8f7094"
      },
      "source": [
        "s='back backing backers better' \n",
        "l = wnl()\n",
        "tex = [ l.lemmatize(x,pos='a') for x in s.split() ]\n",
        "print(tex)\n",
        "\n",
        "p = PS()\n",
        "tex = [ p.stem(x) for x in tex ]\n",
        "t = [ \" \".join(c for c in tex) ]\n",
        "\n",
        "print(t)\n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['back', 'backing', 'backers', 'good']\n",
            "['back back backer good']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFrCdC06CjLS"
      },
      "source": [
        "#preprocessing\n",
        "\n",
        "def pp(i):\n",
        "  #removing punctuation\n",
        "  t = \"\".join(c for c in i if c not in string.punctuation)\n",
        "\n",
        "  # converting to tokens\n",
        "  t = word_tokenize(t)\n",
        "    \n",
        "  # removing numbers and converting to lowercase\n",
        "  t = [ x.lower() for x in t if x.isalpha() ]\n",
        "    \n",
        "  #removing stopwords and words with length 1 or greater than 14\n",
        "  sw = set(stopwords.words('english'))\n",
        "  tex = [ x for x in t if x not in sw and len(x)>1 and len(x)<15 ]\n",
        "    \n",
        "  #lemmatization\n",
        "  l = wnl()\n",
        "  tex = [ l.lemmatize(x) for x in tex ]\n",
        "  p\n",
        "  #stemming\n",
        "  p = PS()\n",
        "  tex = [ p.stem(x) for x in tex ]\n",
        "  t = [ \" \".join(c for c in tex) ]\n",
        "  \n",
        "  #returning the processed string as a single element list\n",
        "  return t "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDp-tku4M7Ux"
      },
      "source": [
        "#tf-idf vectorisation\n",
        "def tf_idf1(fp,n):\n",
        "  vectorizer = TfidfVectorizer()\n",
        "  response = vectorizer.fit_transform(fp)\n",
        "  fname = vectorizer.get_feature_names()\n",
        "  vc = response[0]\n",
        "\n",
        "  d = {'word':fname,'tfidf':[]}\n",
        "  for i in vc.T.toarray():\n",
        "    d['tfidf'].append(i[0])\n",
        "  \n",
        "  D = pd.DataFrame(d)\n",
        "  D = D.sort_values(by=[\"tfidf\"],ascending=False)\n",
        "  \n",
        "  l = D.head(n)['word']\n",
        "  s = []\n",
        "  for i in l:\n",
        "    s.append(i)\n",
        "\n",
        "  l = D.head(n)['tfidf']\n",
        "  s1 = []\n",
        "  for i in l:\n",
        "    #rounding the tf-idf score to 3 decimals\n",
        "    t = round(i,3)\n",
        "    s1.append(t)\n",
        "  \n",
        "  #returns the top 'n' words and their tf-idf scores\n",
        "  return s,s1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdIkWH9yCqgA"
      },
      "source": [
        "#retrieves filepaths of all files in the mentioned path\n",
        "#preproceeses it and calculates tf-idf\n",
        "def ind(path,n):\n",
        "  fp = []\n",
        "  \n",
        "  print(\"\\nFetching Files...\")\n",
        "  lfld(path,fp)\n",
        "  print(\"\\nFiles fetched from \",path,sep='\\n\\n',end='\\n\\n')\n",
        "  \n",
        "  tfidf = { 'doc' : [] }\n",
        "  \n",
        "  #creates dictionary for words and their tf-idf score\n",
        "  for i in range(n):\n",
        "    tfidf[i] = []\n",
        "  for i in range(n):\n",
        "    tfidf[i+n] = []\n",
        "  \n",
        "  #\n",
        "  for f in fp:\n",
        "    s = readf(f)\n",
        "    s = pp(s)\n",
        "    tfidf['doc'].append(f)\n",
        "    tw,ts = tf_idf1(s,n)\n",
        "    for i in range(10):\n",
        "      tfidf[i].append(tw[i])\n",
        "      tfidf[i+10].append(ts[i])\n",
        "  print(\"Preprocessing completed and tf-idf calculated.\")\n",
        "  return tfidf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzAG8m5oCwbn"
      },
      "source": [
        "def tiv():\n",
        "  try:\n",
        "    path = input(\"Enter your path : \")\n",
        "    os.path.listdir(path)\n",
        "  except: \n",
        "      path = '/content/gdrive/My Drive/np/train/sci.crypt'\n",
        "\n",
        "  try:\n",
        "    n = int(input(\"Enter your index degree (default 10) : \"))\n",
        "    if n<10: n = 10\n",
        "  except:\n",
        "    n = 10\n",
        "\n",
        "  print(\"\\n\\npath chosen :\",path)\n",
        "  print(\"Index degree :\",n)\n",
        "\n",
        "\n",
        "  tfidf = ind(path,n)\n",
        "\n",
        "  #getting doc name and words in 'tfidf' as a DataFrame\n",
        "  df = pd.DataFrame(tfidf)\n",
        "  print('\\nDataFrame created.\\n')\n",
        "\n",
        "  return df,n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etVtM0qgYxhh"
      },
      "source": [
        "#to replace punctuation with spaces in query\n",
        "def replacer(t):\n",
        "  for p in string.punctuation:\n",
        "    t = t.replace(p,' ')\n",
        "  return t"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "He7Voe4UeGpP"
      },
      "source": [
        "#to remove duplicates in query\n",
        "def remdupl(t):\n",
        "  l=[]\n",
        "  for w in t.split(' '):\n",
        "    if w not in l: \n",
        "      l.append(w)\n",
        "  return l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wo1e4o0JZ6tz"
      },
      "source": [
        "#finds synonyms for words in query(if any)\n",
        "def syn(w):\n",
        "  try:\n",
        "    s = wordnet.synsets(w)[0]\n",
        "  except:\n",
        "    return [w]\n",
        "  l = s.lemmas()\n",
        "  x = []\n",
        "  for i in range(len(l)):\n",
        "    x.append(l[i].name())\n",
        "  return x\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJ33dKfV6Dts"
      },
      "source": [
        "#checks for the queried word\n",
        "def check(q,df,n):\n",
        "  x = []\n",
        "  sc = []\n",
        "  for i in range(n):\n",
        "    it = str(i)\n",
        "    l = df[df[it] == q]\n",
        "    for j in l['doc']:\n",
        "      x.append(j)\n",
        "    for j in l[str(i+n)]:\n",
        "      sc.append(j)\n",
        "  return x,sc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCyo7cuw72Jh"
      },
      "source": [
        "#adds the a doc and its score\n",
        "def add(fl,l,score):\n",
        "  for x in range(len(l)):\n",
        "    i = l[x]\n",
        "    j = score[x]\n",
        "    if i in fl:\n",
        "      fl[i]['c'] = fl[i]['c'] + 1\n",
        "      fl[i]['s'] = fl[i]['s'] + j\n",
        "    else:\n",
        "      fl[i] = { 'c' : 1 , 's' : j}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-XRReqa8iQ-"
      },
      "source": [
        "#creates a DataFrame and sorts it accordingly\n",
        "def sq(fl):\n",
        "  d = {'doc':[],'freq':[],'score':[]}\n",
        "  for x in fl:\n",
        "    d['doc'].append(x)\n",
        "    d['freq'].append(fl[x]['c'])\n",
        "    d['score'].append(fl[x]['s'])\n",
        "  FL = pd.DataFrame(d)\n",
        "  FL = FL.sort_values(by=['freq','score'],ascending=[False,False])\n",
        "  l = []\n",
        "  for it in FL['doc']:\n",
        "    l.append(it)\n",
        "  return l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cE2YaGPIF1OP"
      },
      "source": [
        "def ppq(df,a):\n",
        "  if not(a):\n",
        "    df,n = tiv()\n",
        "  else:\n",
        "    try:\n",
        "      n = int(input(\"Enter your index degree (default 10) : \"))\n",
        "      if n<10: n = 10\n",
        "    except:\n",
        "      n = 10\n",
        "  qf=[]\n",
        "  g=''\n",
        "  q = input(\"Enter query/filepath :\\n\")\n",
        "  if os.path.isfile(q):\n",
        "    q = readf(q)\n",
        "    \n",
        "  #replacing punctuation\n",
        "  q = replacer(q)\n",
        "    \n",
        "  #preprocessing query  \n",
        "  q = pp(q)[0]\n",
        "    \n",
        "  #removing duplicates\n",
        "  q = remdupl(q)\n",
        "\n",
        "  #finding synonyms(if any)\n",
        "  for i in q:\n",
        "    for s in syn(i):\n",
        "      #qf.append(s)\n",
        "      g = g+s+' '\n",
        "\n",
        "  #preprocessing and duplicates again\n",
        "  qfg = pp(g)\n",
        "  qf = remdupl(qfg[0])\n",
        "  return qf,df,n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DK1PerkQCcIF"
      },
      "source": [
        "def qfind(qf,df,n):\n",
        "  fl = {}\n",
        "  for word in qf:\n",
        "    l,score = check(word,df,n)\n",
        "    add(fl,l,score)\n",
        "  sl = sq(fl)\n",
        "  return sl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4BYtAasMd98"
      },
      "source": [
        "def qagg(sl):\n",
        "  l = len(sl)\n",
        "  i = 0\n",
        "  mi = l//10\n",
        "  ch = a = '<'\n",
        "  b = '>'\n",
        "  while ch in [a,b]:\n",
        "    if ch == '<':\n",
        "      if i!=0:\n",
        "        i-=1\n",
        "    else:\n",
        "      if i!=mi:i+=1\n",
        "    print('\\nPage ',i+1,'of',mi+1,end='\\n\\n')\n",
        "    si = i*10\n",
        "    ei = si+10\n",
        "    if ei>l:\n",
        "      ei = l\n",
        "    for x in sl[si:ei]:print(x)\n",
        "    if i!=0:\n",
        "      print('<',end=' ')\n",
        "      a='<'\n",
        "    else:\n",
        "      a = None\n",
        "    if i!=mi:\n",
        "      print('>')\n",
        "      b='>'\n",
        "    else:\n",
        "      b=None\n",
        "    ch = input()\n",
        "  else:\n",
        "    print(\"Thank You!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtMoBADtzDu1"
      },
      "source": [
        "To index a document"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IG2X0iA5NLyl",
        "outputId": "6b949e19-4da9-45c5-e57a-f83ca92c2060",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        }
      },
      "source": [
        "#To index a document specifically\n",
        "#fp = '/content/gdrive/My Drive/np/train/sci.crypt/14147'\n",
        "try:\n",
        "  fp = input(\"Enter path : \")\n",
        "  f = open(fp,'r')\n",
        "except:\n",
        "  fp = '/content/gdrive/My Drive/np/train/sci.crypt/14147'\n",
        "\n",
        "try:\n",
        "  n = int(input(\"Enter your index degree (default 10) : \"))\n",
        "  if n<10: n = 10\n",
        "except:\n",
        "  n = 10\n",
        "print(\"\\n\\nFile chosen :\",fp,end='\\n\\n')\n",
        "#reading the file path and preprocesses the content\n",
        "fp = readf(fp)\n",
        "fp = pp(fp)\n",
        "\n",
        "#tf-idf vectorisation is done\n",
        "x,y = tf_idf1(fp,n)\n",
        "\n",
        "#prints the corresponding indexes\n",
        "print('The indices are : ')\n",
        "for i in x :print(i)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter path : \n",
            "Enter your index degree (default 10) : 8\n",
            "\n",
            "\n",
            "File chosen : /content/gdrive/My Drive/np/train/sci.crypt/14147\n",
            "\n",
            "The indices are : \n",
            "ripem\n",
            "key\n",
            "rsa\n",
            "pgp\n",
            "use\n",
            "public\n",
            "messag\n",
            "de\n",
            "encrypt\n",
            "mail\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIxE4OkO3ECn"
      },
      "source": [
        "To index a list of documents in a folder/directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sgw4x0qu-VrN",
        "outputId": "4432fb07-3185-4083-99e6-617095372a84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        }
      },
      "source": [
        "print(\"To index a list of documents in a folder/directory\")\n",
        "\n",
        "df,n = tiv()\n",
        "\n",
        "#saving 'tfidf' to csv file\n",
        "try:\n",
        "  cp = input(\"Enter csv file path : \")\n",
        "  df.to_csv(cp, index = False )\n",
        "except:\n",
        "  df.to_csv(r'/content/gdrive/My Drive/np/i.csv' , index = False )\n",
        "\n",
        "print('Added content to csv file.')\n",
        "try:\n",
        "  print(cp)\n",
        "except: \n",
        "  print('\\n/content/gdrive/My Drive/np/i.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "To index a list of documents in a folder/directory\n",
            "Enter your path : \n",
            "Enter your index degree (default 10) : 9\n",
            "\n",
            "\n",
            "path chosen : /content/gdrive/My Drive/np/train/sci.crypt\n",
            "Index degree : 10\n",
            "Fetching Files...\n",
            "Files fetched from  /content/gdrive/My Drive/np/train/sci.crypt\n",
            "\n",
            "Preprocessing completed and tf-idf calculated.\n",
            "\n",
            "DataFrame created.\n",
            "\n",
            "Enter csv file path : i.csv\n",
            "Added content to csv file.\n",
            "i.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74EFGyntD9dR"
      },
      "source": [
        "To query from a list of documents in folder/directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFm6u8PUDhqu",
        "outputId": "f6cf40aa-4a45-4fc6-dff9-953b5a495d20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 907
        }
      },
      "source": [
        "print(\"For query :\\n\")\n",
        "cp = input(\"Enter your csv file :\\n\")\n",
        "try:\n",
        "  df = pd.read_csv(cp)\n",
        "  qf,df,n = ppq(df,True)\n",
        "except:\n",
        "  qf,df,n = ppq([],False)\n",
        "sl = qfind(qf,df,n)\n",
        "qagg(sl)\n",
        "#rob key without id?hood"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "For query :\n",
            "\n",
            "Enter your csv file :\n",
            "i.csv\n",
            "Enter your index degree (default 10) : 2\n",
            "Enter query/filepath :\n",
            "key chip id\n",
            "\n",
            "Page  1 of 25\n",
            "\n",
            "/content/gdrive/My Drive/np/train/sci.crypt/15620\n",
            "/content/gdrive/My Drive/np/train/sci.crypt/15498\n",
            "/content/gdrive/My Drive/np/train/sci.crypt/15495\n",
            "/content/gdrive/My Drive/np/train/sci.crypt/15426\n",
            "/content/gdrive/My Drive/np/train/sci.crypt/15442\n",
            "/content/gdrive/My Drive/np/train/sci.crypt/15502\n",
            "/content/gdrive/My Drive/np/train/sci.crypt/15700\n",
            "/content/gdrive/My Drive/np/train/sci.crypt/15301\n",
            "/content/gdrive/My Drive/np/train/sci.crypt/15670\n",
            "/content/gdrive/My Drive/np/train/sci.crypt/15455\n",
            ">\n",
            ">\n",
            "\n",
            "Page  2 of 25\n",
            "\n",
            "/content/gdrive/My Drive/np/train/sci.crypt/15640\n",
            "/content/gdrive/My Drive/np/train/sci.crypt/15375\n",
            "/content/gdrive/My Drive/np/train/sci.crypt/15706\n",
            "/content/gdrive/My Drive/np/train/sci.crypt/15658\n",
            "/content/gdrive/My Drive/np/train/sci.crypt/15693\n",
            "/content/gdrive/My Drive/np/train/sci.crypt/15374\n",
            "/content/gdrive/My Drive/np/train/sci.crypt/15548\n",
            "/content/gdrive/My Drive/np/train/sci.crypt/15343\n",
            "/content/gdrive/My Drive/np/train/sci.crypt/15378\n",
            "/content/gdrive/My Drive/np/train/sci.crypt/15296\n",
            "< >\n",
            "<\n",
            "\n",
            "Page  1 of 25\n",
            "\n",
            "/content/gdrive/My Drive/np/train/sci.crypt/15620\n",
            "/content/gdrive/My Drive/np/train/sci.crypt/15498\n",
            "/content/gdrive/My Drive/np/train/sci.crypt/15495\n",
            "/content/gdrive/My Drive/np/train/sci.crypt/15426\n",
            "/content/gdrive/My Drive/np/train/sci.crypt/15442\n",
            "/content/gdrive/My Drive/np/train/sci.crypt/15502\n",
            "/content/gdrive/My Drive/np/train/sci.crypt/15700\n",
            "/content/gdrive/My Drive/np/train/sci.crypt/15301\n",
            "/content/gdrive/My Drive/np/train/sci.crypt/15670\n",
            "/content/gdrive/My Drive/np/train/sci.crypt/15455\n",
            ">\n",
            "exit\n",
            "Thank You!\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}